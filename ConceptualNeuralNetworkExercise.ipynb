{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConceptualNeuralNetworkExercise.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPceS0c8/B6l24Wqgql2S3m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TanChen168/Week11_IntroductionToDeepLearning/blob/main/ConceptualNeuralNetworkExercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the network. What do you notice?\n",
        "\n",
        "-- The network doesn't recognize the pattern with learning rate of 0.01 and Linear Activation"
      ],
      "metadata": {
        "id": "HiKxyPzSPUrH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Increase the number of neurons in the hidden layer and try changing the activation function to something other than \"Linear.\" Can you model the data now? How many neurons and what activation function allows you to effectively model the data?\n",
        "\n",
        "-- Increasing neurons to 3 in the hidden layer allows RELU, Tanh and Sigmoid activation function to model the data. With RELU having sharp edges while Tanh and Sigmoid has rounded borders of the data modeled."
      ],
      "metadata": {
        "id": "8rqr5KZAPWcb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Can you model the data so that the testing loss is 0.01 or less? What is the smallest number of neurons and layers you can use that gives a test loss of 0.01 or less?\n",
        "\n",
        "-- 1 hidden layer and 3 neurons at 0.01 Learning Rate using RELU (865+ epoch) and Sigmoid (3,700+ epoch) Activation can give test lost of 0.01 or less\n"
      ],
      "metadata": {
        "id": "9_wrwmrFPe95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Play around with the learning rate. What do you notice? Based on this, what do you think the learning rate is?\n",
        "\n",
        "-- Learning rate of 0.1 performs better than the default 0.01."
      ],
      "metadata": {
        "id": "VJs5i3jUPkIW"
      }
    }
  ]
}